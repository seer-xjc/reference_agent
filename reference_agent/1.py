import os
import gradio as gr
from pathlib import Path
import tempfile
from collections import Counter
from docx import Document
import fitz
from dotenv import load_dotenv
from zhipuai import ZhipuAI
from utils import (
    get_reference_titles,
    get_citation_markers,
    search_from_arxiv,
    get_arxiv_metadata_only,
    batch_verify_citations_lightweight,
    load_pdf,
    load_prompt
)
from difflib import SequenceMatcher
import time

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


def clean_title_for_comparison(title):
    """æ¸…ç†æ ‡é¢˜ç”¨äºæ¯”è¾ƒï¼Œå»é™¤æ ‡ç‚¹ç¬¦å·ã€è½¬æ¢ä¸ºå°å†™ç­‰"""
    import re
    cleaned = re.sub(r'[^\w\s]', ' ', title.lower())
    cleaned = re.sub(r'\s+', ' ', cleaned).strip()
    return cleaned


def is_similar(title1, title2, threshold=0.8):
    """æ”¹è¿›çš„ç›¸ä¼¼æ€§æ¯”è¾ƒå‡½æ•°"""
    similarity1 = SequenceMatcher(None, title1.lower(), title2.lower()).ratio()
    clean_title1 = clean_title_for_comparison(title1)
    clean_title2 = clean_title_for_comparison(title2)
    similarity2 = SequenceMatcher(None, clean_title1, clean_title2).ratio()
    words1 = set(clean_title1.split())
    words2 = set(clean_title2.split())
    if len(words1) > 0 and len(words2) > 0:
        word_overlap = len(words1.intersection(words2)) / len(words1.union(words2))
    else:
        word_overlap = 0
    max_similarity = max(similarity1, similarity2, word_overlap)
    return max_similarity > threshold, max_similarity


def load_document(file_path):
    """æ”¯æŒåŠ è½½word/pdfæ–‡æ¡£"""
    if file_path.endswith('.pdf'):
        with fitz.open(file_path) as doc:
            return "\n".join([page.get_text() for page in doc])
    else:
        return "\n".join([p.text for p in Document(file_path).paragraphs if p.text])


def verify_citations_and_analyze_with_logs(file_path, lightweight_mode, skip_download, pdf_verify):
    """ä¸»è¦çš„æ–‡çŒ®æ£€æŸ¥å‡½æ•°ï¼Œæ•´åˆæ‰€æœ‰åŠŸèƒ½ï¼Œå¸¦å®æ—¶æ—¥å¿—"""
    results = {
        'doc_content': '',
        'titles': [],
        'citations_info': {},
        'arxiv_found': [],
        'arxiv_not_found': [],
        'verification_results': []
    }
    log_messages = []

    def add_log(message):
        """æ·»åŠ æ—¥å¿—æ¶ˆæ¯"""
        log_messages.append(f"[{time.strftime('%H:%M:%S')}] {message}")
        return "\n".join(log_messages)

    try:
        yield add_log("ğŸš€ å¼€å§‹æ–‡çŒ®æ£€æŸ¥åˆ†æ..."), "", "", "", "", "åˆ†æä¸­..."
        add_log(f"ğŸ“– æ­£åœ¨åŠ è½½æ–‡æ¡£: {Path(file_path).name}")
        doc_content = load_document(file_path)
        results['doc_content'] = doc_content
        add_log(f"âœ… æ–‡æ¡£åŠ è½½å®Œæˆï¼Œå…± {len(doc_content)} ä¸ªå­—ç¬¦")
        yield add_log(""), "", "", "", "", "åˆ†æä¸­..."

        add_log("ğŸ” æ­£åœ¨æå–å‚è€ƒæ–‡çŒ®æ ‡é¢˜...")
        titles = get_reference_titles(doc_content)
        results['titles'] = titles
        add_log(f"ğŸ“š æå–åˆ° {len(titles)} ç¯‡å‚è€ƒæ–‡çŒ®")
        yield add_log(""), "", "", "", "", "åˆ†æä¸­..."

        add_log("ğŸ“Š æ­£åœ¨åˆ†æå¼•æ–‡æ ‡è®°...")
        citations_to_text = get_citation_markers(doc_content)
        add_log(f"ğŸ”– æ‰¾åˆ° {len(citations_to_text)} ä¸ªå¼•ç”¨æ ‡è®°")

        citations = []
        for citation, citation_text in citations_to_text:
            citations.extend(citation)

        missed_citations = list(set(range(1, len(titles) + 1)) - set(citations))
        counter = Counter(citations)
        duplicate_citations = [citation for citation, count in counter.items() if count > 1]

        results['citations_info'] = {
            'total_references': len(titles),
            'total_citations': len(citations_to_text),
            'unique_citations': len(set(citations)),
            'missed_citations': missed_citations,
            'duplicate_citations': duplicate_citations,
            'citation_details': [(citation, count) for citation, count in counter.items() if count > 1]
        }

        add_log(f"ğŸ“ˆ å¼•æ–‡ç»Ÿè®¡å®Œæˆ:")
        add_log(f"   - æ€»å¼•ç”¨æ•°: {len(set(citations))} ä¸ª")
        add_log(f"   - æœªå¼•ç”¨æ–‡çŒ®: {len(missed_citations)} ç¯‡")
        add_log(f"   - é‡å¤å¼•ç”¨: {len(duplicate_citations)} ç¯‡")

        citation_analysis = format_citation_analysis(results)
        yield add_log(""), citation_analysis, "", "", "", "åˆ†æä¸­..."

        add_log("ğŸŒ å¼€å§‹åœ¨arXivä¸­æœç´¢æ–‡çŒ®...")
        arxiv_found = []
        arxiv_not_found = []

        for i, title in enumerate(titles):
            add_log(f"ğŸ“„ å¤„ç†æ–‡çŒ® {i + 1}/{len(titles)}: {title[:50]}{'...' if len(title) > 50 else ''}")
            yield add_log(""), citation_analysis, "", "", "", f"æ­£åœ¨æœç´¢æ–‡çŒ® {i + 1}/{len(titles)}..."

            try:
                add_log(f"ğŸ” åœ¨arXivæœç´¢: {title[:30]}...")
                search_results = list(search_from_arxiv(title))
                found_match = False

                if search_results:
                    add_log(f"   ğŸ“š æ‰¾åˆ° {len(search_results)} ä¸ªæœç´¢ç»“æœ")
                    for j, result in enumerate(search_results):
                        add_log(f"   ğŸ” æ£€æŸ¥ç»“æœ {j + 1}: {result.title[:40]}...")
                        is_match, similarity = is_similar(result.title, title)
                        add_log(f"      ç›¸ä¼¼åº¦: {similarity:.3f}")

                        if is_match:
                            add_log(f"   âœ… æ‰¾åˆ°åŒ¹é…! ç›¸ä¼¼åº¦: {similarity:.3f}")
                            arxiv_found.append({
                                'index': i + 1,
                                'title': title,
                                'arxiv_title': result.title,
                                'similarity': similarity,
                                'authors': [str(author) for author in result.authors],
                                'abstract': result.summary[:200] + "..." if len(
                                    result.summary) > 200 else result.summary
                            })
                            found_match = True
                            break
                        else:
                            add_log(f"      âŒ ç›¸ä¼¼åº¦ä¸è¶³ (é˜ˆå€¼: 0.8)")

                if not found_match:
                    add_log(f"   âŒ æœªæ‰¾åˆ°åŒ¹é…çš„æ–‡çŒ®")
                    arxiv_not_found.append({
                        'index': i + 1,
                        'title': title
                    })

            except Exception as e:
                add_log(f"   âŒ æœç´¢å‡ºé”™: {str(e)[:50]}...")
                arxiv_not_found.append({
                    'index': i + 1,
                    'title': title,
                    'error': str(e)
                })

            if (i + 1) % 5 == 0 or i == len(titles) - 1:
                results['arxiv_found'] = arxiv_found
                results['arxiv_not_found'] = arxiv_not_found
                arxiv_found_text, arxiv_not_found_text = format_arxiv_analysis(results)
                yield add_log(
                    ""), citation_analysis, arxiv_found_text, arxiv_not_found_text, "", f"å·²å¤„ç† {i + 1}/{len(titles)} ç¯‡æ–‡çŒ®"

        results['arxiv_found'] = arxiv_found
        results['arxiv_not_found'] = arxiv_not_found
        add_log(f"ğŸ“Š arXivæœç´¢å®Œæˆ:")
        add_log(f"   - æ‰¾åˆ°åŒ¹é…: {len(arxiv_found)} ç¯‡")
        add_log(f"   - æœªæ‰¾åˆ°: {len(arxiv_not_found)} ç¯‡")

        add_log("ğŸ¤– å¼€å§‹AIç›¸å…³æ€§éªŒè¯...")
        if lightweight_mode:
            add_log("   ä½¿ç”¨è½»é‡çº§æ¨¡å¼: arXivå…ƒæ•°æ®éªŒè¯")
        else:
            add_log("   ä½¿ç”¨æ ‡å‡†æ¨¡å¼: PDFå†…å®¹éªŒè¯")
        yield add_log(""), citation_analysis, format_arxiv_analysis(results)[0], format_arxiv_analysis(results)[
            1], "", "æ­£åœ¨è¿›è¡ŒAIéªŒè¯..."

        add_log(f"   æ­£åœ¨éªŒè¯ {len(citations_to_text)} ä¸ªå¼•ç”¨æ ‡è®°...")
        verification_results = []

        if lightweight_mode:
            for i, (citation, text) in enumerate(citations_to_text):
                add_log(f"   ğŸ” éªŒè¯å¼•ç”¨ {i + 1}/{len(citations_to_text)}: {citation}")
                single_verification = batch_verify_citations_lightweight([(citation, text)], titles, "glm-4-flash")
                verification_results.extend(single_verification)

                if single_verification and single_verification[0]['status'] == 'verified':
                    result_text = single_verification[0].get('result', '')
                    if '<æ˜¯>' in result_text:
                        add_log(f"      âœ… éªŒè¯é€šè¿‡")
                    elif 'å¦' in result_text:
                        reason = result_text.replace('<å¦:', '').replace('>', '').strip()
                        add_log(f"      âŒ éœ€è¦æ£€æŸ¥: {reason[:50]}...")
                    else:
                        add_log(f"      âš ï¸  ç»“æœä¸æ˜ç¡®")
                else:
                    add_log(f"      â­ï¸  è·³è¿‡éªŒè¯")

                if (i + 1) % 3 == 0 or i == len(citations_to_text) - 1:
                    yield add_log(""), citation_analysis, format_arxiv_analysis(results)[0], \
                    format_arxiv_analysis(results)[1], "", f"å·²éªŒè¯ {i + 1}/{len(citations_to_text)} ä¸ªå¼•ç”¨"
        else:
            pass

        add_log("ğŸ‰ æ–‡çŒ®æ£€æŸ¥åˆ†æå®Œæˆï¼")
        yield add_log(""), citation_analysis, format_arxiv_analysis(results)[0], format_arxiv_analysis(results)[
            1], "", "åˆ†æå®Œæˆ"

    except Exception as e:
        add_log(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)[:50]}...")
        yield add_log(""), "", "", "", "", f"åˆ†æå¤±è´¥: {str(e)[:50]}..."


def format_citation_analysis(results):
    """æ ¼å¼åŒ–å¼•æ–‡åˆ†æç»“æœ"""
    info = results['citations_info']
    output = []
    output.append("ğŸ“Š å¼•æ–‡åˆ†æç»“æœï¼š")
    output.append(f"- æ€»å‚è€ƒæ–‡çŒ®æ•°: {info['total_references']}")
    output.append(f"- æ€»å¼•ç”¨æ ‡è®°æ•°: {info['total_citations']}")
    output.append(f"- å”¯ä¸€å¼•ç”¨ç¼–å·æ•°: {info['unique_citations']}")

    if info['missed_citations']:
        output.append(f"- æœªè¢«å¼•ç”¨çš„æ–‡çŒ®: {len(info['missed_citations'])} ç¯‡")
        for citation in sorted(info['missed_citations']):
            output.append(f"  - æ–‡çŒ®[{citation}] æœªè¢«å¼•ç”¨")
    else:
        output.append("- æ‰€æœ‰æ–‡çŒ®å‡è¢«å¼•ç”¨ âœ…")

    if info['duplicate_citations']:
        output.append(f"- é‡å¤å¼•ç”¨çš„æ–‡çŒ®: {len(info['duplicate_citations'])} ç¯‡")
        for citation, count in info['citation_details']:
            output.append(f"  - æ–‡çŒ®[{citation}] è¢«å¼•ç”¨ {count} æ¬¡")
    else:
        output.append("- æ— é‡å¤å¼•ç”¨ âœ…")

    return "\n".join(output)


def format_arxiv_analysis(results):
    """æ ¼å¼åŒ–arXivæœç´¢ç»“æœ"""
    found_output = ["ğŸ“š arXivæ‰¾åˆ°çš„æ–‡çŒ®ï¼š"]
    for item in results['arxiv_found']:
        found_output.append(f"- æ–‡çŒ®[{item['index']}]: {item['title'][:50]}...")
        found_output.append(f"  - arXivæ ‡é¢˜: {item['arxiv_title'][:50]}...")
        found_output.append(f"  - ç›¸ä¼¼åº¦: {item['similarity']:.3f}")
        found_output.append(f"  - ä½œè€…: {', '.join(item['authors'][:3])}{'...' if len(item['authors']) > 3 else ''}")
        found_output.append(f"  - æ‘˜è¦: {item['abstract'][:100]}...")

    not_found_output = ["âŒ arXivæœªæ‰¾åˆ°çš„æ–‡çŒ®ï¼š"]
    for item in results['arxiv_not_found']:
        not_found_output.append(f"- æ–‡çŒ®[{item['index']}]: {item['title'][:50]}...")
        if 'error' in item:
            not_found_output.append(f"  - é”™è¯¯: {item['error'][:50]}...")

    return "\n".join(found_output), "\n".join(not_found_output)


def submit_feedback(feedback):
    """å¤„ç†ç”¨æˆ·åé¦ˆå¹¶ä¿å­˜åˆ°æ–‡ä»¶"""
    if not feedback.strip():
        return "âš ï¸ åé¦ˆå†…å®¹ä¸èƒ½ä¸ºç©ºï¼"

    feedback_dir = Path("feedback")
    feedback_dir.mkdir(exist_ok=True)
    feedback_file = feedback_dir / "feedback.txt"

    try:
        with open(feedback_file, 'a', encoding='utf-8') as f:
            timestamp = time.strftime('%Y-%m-%d %H:%M:%S')
            f.write(f"[{timestamp}] {feedback}\n")
        return "âœ… æ„Ÿè°¢æ‚¨çš„åé¦ˆï¼å·²æˆåŠŸä¿å­˜ã€‚"
    except Exception as e:
        return f"âŒ ä¿å­˜åé¦ˆå¤±è´¥: {str(e)[:50]}..."


with gr.Blocks() as demo:
    gr.Markdown("ğŸ“š æ–‡çŒ®å¼•ç”¨æŸ¥è¯æ™ºèƒ½ä½“")
    with gr.Row():
        with gr.Column(scale=2):
            file_input = gr.File(label="ä¸Šä¼ PDFæˆ–DOCXæ–‡ä»¶", file_types=[".pdf", ".docx"])
            lightweight_checkbox = gr.Checkbox(label="è½»é‡çº§æ¨¡å¼ (ä½¿ç”¨arXivå…ƒæ•°æ®)", value=True)
            skip_download_checkbox = gr.Checkbox(label="è·³è¿‡PDFä¸‹è½½", value=False)
            pdf_verify_checkbox = gr.Checkbox(label="è·³è¿‡PDFå†…å®¹éªŒè¯", value=False)
            submit_button = gr.Button("å¼€å§‹æ£€æŸ¥", variant="primary")
        with gr.Column(scale=1):
            gr.Markdown("### æä¾›åé¦ˆ")
            feedback_input = gr.Textbox(
                label="è¯·ç•™ä¸‹æ‚¨çš„åé¦ˆ",
                lines=4,
                placeholder="è¯·è¾“å…¥æ‚¨å¯¹å·¥å…·çš„å»ºè®®æˆ–é—®é¢˜æè¿°..."
            )
            feedback_submit_button = gr.Button("æäº¤åé¦ˆ", variant="secondary")
            feedback_result = gr.Textbox(label="åé¦ˆç»“æœ", interactive=False)

    log_output = gr.Textbox(label="è¿è¡Œæ—¥å¿—", lines=10, interactive=False)
    citation_analysis_output = gr.Textbox(label="å¼•æ–‡åˆ†æç»“æœ", lines=8, interactive=False)
    arxiv_found_output = gr.Textbox(label="arXivæ‰¾åˆ°çš„æ–‡çŒ®", lines=8, interactive=False)
    arxiv_not_found_output = gr.Textbox(label="arXivæœªæ‰¾åˆ°çš„æ–‡çŒ®", lines=8, interactive=False)
    status_output = gr.Textbox(label="çŠ¶æ€", interactive=False)

    submit_button.click(
        verify_citations_and_analyze_with_logs,
        inputs=[file_input, lightweight_checkbox, skip_download_checkbox, pdf_verify_checkbox],
        outputs=[log_output, citation_analysis_output, arxiv_found_output, arxiv_not_found_output, status_output]
    )

    feedback_submit_button.click(
        submit_feedback,
        inputs=feedback_input,
        outputs=feedback_result
    )

demo.launch()