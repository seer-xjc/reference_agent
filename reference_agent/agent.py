import os
from pathlib import Path
import argparse
from collections import Counter
from docx import Document
from dotenv import load_dotenv
from zhipuai import ZhipuAI
import fitz
import re
import logging
from utils import (
    load_prompt,
    normalize_doc,
    get_reference_titles,
    get_citation_markers,
    load_pdf,
    search_from_arxiv,
    batch_verify_citations_lightweight
)
from difflib import SequenceMatcher
import time

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

def safe_download(result, title, i, max_retries=3):
    for attempt in range(max_retries):
        try:
            print(f"å°è¯•ä¸‹è½½: {title} (ç¬¬ {attempt+1} æ¬¡)")
            result.download_pdf(dirpath="../data/references", filename=f"{i+1}.pdf")
            print(f"âœ… ä¸‹è½½æˆåŠŸ: {title}")
            return True
        except Exception as e:
            print(f"âŒ ä¸‹è½½å¤±è´¥: {title}ï¼Œé”™è¯¯: {e}")
            time.sleep(2)  # ç®€å•é€€é¿
    print(f"ğŸš« æ”¾å¼ƒä¸‹è½½: {title}")
    return False  # æ˜ç¡®è¡¨ç¤ºå¤±è´¥


def clean_title_for_comparison(title):
    """æ¸…ç†æ ‡é¢˜ç”¨äºæ¯”è¾ƒï¼Œå»é™¤æ ‡ç‚¹ç¬¦å·ã€è½¬æ¢ä¸ºå°å†™ç­‰"""
    # å»é™¤å¸¸è§çš„æ ‡ç‚¹ç¬¦å·å’Œç‰¹æ®Šå­—ç¬¦
    cleaned = re.sub(r'[^\w\s]', ' ', title.lower())
    # å»é™¤å¤šä½™ç©ºæ ¼
    cleaned = re.sub(r'\s+', ' ', cleaned).strip()
    return cleaned


def is_similar(title1, title2, threshold=0.8):
    """æ”¹è¿›çš„ç›¸ä¼¼æ€§æ¯”è¾ƒå‡½æ•°"""
    # åŸå§‹æ ‡é¢˜æ¯”è¾ƒ
    similarity1 = SequenceMatcher(None, title1.lower(), title2.lower()).ratio()
    
    # æ¸…ç†åçš„æ ‡é¢˜æ¯”è¾ƒ
    clean_title1 = clean_title_for_comparison(title1)
    clean_title2 = clean_title_for_comparison(title2)
    similarity2 = SequenceMatcher(None, clean_title1, clean_title2).ratio()
    
    # å…³é”®è¯åŒ¹é…
    words1 = set(clean_title1.split())
    words2 = set(clean_title2.split())
    if len(words1) > 0 and len(words2) > 0:
        word_overlap = len(words1.intersection(words2)) / len(words1.union(words2))
    else:
        word_overlap = 0
    
    # ç»¼åˆè¯„åˆ†ï¼ˆå–æœ€é«˜åˆ†ï¼‰
    max_similarity = max(similarity1, similarity2, word_overlap)
    
    print(f"    ğŸ“Š ç›¸ä¼¼åº¦åˆ†æ:")
    print(f"       åŸå§‹: {similarity1:.3f}")
    print(f"       æ¸…ç†: {similarity2:.3f}")
    print(f"       è¯æ±‡: {word_overlap:.3f}")
    print(f"       æœ€ç»ˆ: {max_similarity:.3f} (é˜ˆå€¼: {threshold})")
    
    return max_similarity > threshold


class Agent:
    def __init__(self, model, prompt, doc, ref):
        self.model = model
        self.prompt = load_prompt(prompt)
        self.doc = self._load_document(doc)  # ä¿®æ”¹ä¸ºç»Ÿä¸€åŠ è½½æ–¹æ³•
        self.ref = ref

    def _load_document(self, doc_path):
        """æ”¯æŒåŠ è½½word/pdfæ–‡æ¡£"""
        if doc_path.endswith('.pdf'):
            with fitz.open(doc_path) as doc:
                return "\n".join([page.get_text() for page in doc])
        else:
            return "\n".join([p.text for p in Document(doc_path).paragraphs if p.text])

    def call_model(self, model, prompt):
        client = ZhipuAI(api_key=os.environ.get("ZHIPUAI_API_KEY"))
        response = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0,
            stream=False
        )
        return response.choices[0].message.content

    def verify_citations_referenced(self):
        print("ç¬¬1æ­¥: æ ¸æŸ¥å¼•æ–‡æ•°é‡åŠå¼•ç”¨æƒ…å†µ")
        titles = get_reference_titles(self.doc)
        print(f"æ–‡çŒ®åˆ—è¡¨å…±æœ‰{len(titles)}ç¯‡å‚è€ƒæ–‡çŒ®")

        citations = []
        citations_to_text = get_citation_markers(self.doc)
        print(f"æå–åˆ°{len(citations_to_text)}ä¸ªå¼•ç”¨æ ‡è®°")
        
        for i, (citation, citation_text) in enumerate(citations_to_text):
            print(f"å¼•ç”¨æ ‡è®° {i+1}: {citation} -> {citation_text[:50]}...")
            citations.extend(citation)

        print(f"æ€»å…±æ‰¾åˆ°{len(citations)}ä¸ªå¼•ç”¨ç¼–å·: {sorted(set(citations))}")
        
        missed_citations = list(set(range(1, len(titles)+1)) - set(citations))
        if missed_citations:
            print(f"ä»¥ä¸‹{len(missed_citations)}ä¸ªæ–‡çŒ®æ²¡æœ‰è¢«å¼•ç”¨:")
            for citation in sorted(missed_citations):
                print(f"æ–‡çŒ®[{citation}]æ²¡æœ‰è¢«å¼•ç”¨")
        else:
            print("æ‰€æœ‰æ–‡çŒ®éƒ½è¢«å¼•ç”¨äº† âœ…")

        counter = Counter(citations)
        duplicate_count = 0
        for citation, count in counter.items():
            if count > 1:
                duplicate_count += 1
                print(f"æ–‡çŒ®[{citation}]çš„å¼•ç”¨è¶…è¿‡1æ¬¡,å…±å¼•ç”¨äº†{count}æ¬¡")
        
        if duplicate_count == 0:
            print("æ²¡æœ‰é‡å¤å¼•ç”¨çš„æ–‡çŒ® âœ…")

    def download_literatures(self):
        print("ç¬¬2æ­¥: å¼€å§‹ä¸‹è½½æ–‡çŒ®:")
        titles = get_reference_titles(self.doc)
        failed_titles = []
        skipped_titles = []
        
        # ç¡®ä¿ä¸‹è½½ç›®å½•å­˜åœ¨
        download_dir = Path("../data/references")
        download_dir.mkdir(parents=True, exist_ok=True)

        for i, title in enumerate(titles):
            pdf_filename = f"{i+1}.pdf"
            pdf_path = download_dir / pdf_filename
            
            print(f"\nğŸ“„ å¤„ç†æ–‡çŒ® {i+1}/{len(titles)}: {title}")
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²ç»å­˜åœ¨
            if pdf_path.exists():
                print(f"â­ï¸  è·³è¿‡å·²å­˜åœ¨çš„æ–‡çŒ® (æ–‡ä»¶: {pdf_filename})")
                skipped_titles.append(title)
                continue
            
            found = False
            print(f"ğŸ” åœ¨arXivæœç´¢: {title}")
            
            try:
                search_results = list(search_from_arxiv(title))
                print(f"   ğŸ“š æ‰¾åˆ° {len(search_results)} ä¸ªæœç´¢ç»“æœ")
                
                if not search_results:
                    print(f"âŒ arXivä¸Šæ— æœç´¢ç»“æœ")
                    failed_titles.append(title)
                    continue
                
                # éå†æœç´¢ç»“æœå¯»æ‰¾æœ€ä½³åŒ¹é…
                best_match = None
                best_similarity = 0
                
                for j, result in enumerate(search_results):
                    print(f"   ğŸ” æ£€æŸ¥ç»“æœ {j+1}: {result.title}")
                    
                    if is_similar(result.title, title):
                        found = True
                        print(f"   âœ… æ‰¾åˆ°åŒ¹é…!")
                        success = safe_download(result, title, i)
                        if success:
                            break
                        else:
                            found = False
                            
                if not found:
                    print(f"âŒ æœªæ‰¾åˆ°è¶³å¤Ÿç›¸ä¼¼çš„æ–‡çŒ®")
                    failed_titles.append(title)
                    
            except Exception as e:
                print(f"âŒ æœç´¢è¿‡ç¨‹å‡ºé”™: {str(e)}")
                failed_titles.append(title)

        print("\n" + "="*60)
        print("ğŸ“Š ä¸‹è½½ç»Ÿè®¡:")
        
        # ç»Ÿè®¡ç»“æœ
        total_titles = len(titles)
        skipped_count = len(skipped_titles)
        failed_count = len(failed_titles)
        success_count = total_titles - skipped_count - failed_count
        
        print(f"   ğŸ“š æ€»æ–‡çŒ®æ•°: {total_titles}")
        print(f"   â­ï¸  å·²å­˜åœ¨(è·³è¿‡): {skipped_count}")
        print(f"   âœ… æ–°ä¸‹è½½æˆåŠŸ: {success_count}")
        print(f"   âŒ ä¸‹è½½å¤±è´¥: {failed_count}")
        
        if skipped_count > 0:
            print(f"\nâ­ï¸  è·³è¿‡çš„æ–‡çŒ®:")
            for i, title in enumerate(skipped_titles, 1):
                print(f"   {i}. {title}")
        
        if failed_count > 0:
            print(f"\nâŒ ä¸‹è½½å¤±è´¥çš„æ–‡çŒ®:")
            for i, title in enumerate(failed_titles, 1):
                print(f"   {i}. {title}")
        
        if failed_count == 0:
            if success_count > 0:
                print(f"\nğŸ‰ æ–°ä¸‹è½½çš„ {success_count} ç¯‡æ–‡çŒ®å…¨éƒ¨æˆåŠŸ!")
            if skipped_count + success_count == total_titles:
                print("ğŸ‰ æ‰€æœ‰æ–‡çŒ®éƒ½å·²å‡†å¤‡å°±ç»ª!")
        
        print("="*60)

    def verify_citation_sentences(self):
        print("ç¬¬3æ­¥: æ ¸æŸ¥å¼•æ–‡ä¸æ–‡çŒ®çš„å¯¹åº”å…³ç³»")
        bad_count = 0
        checked_count = 0
        skipped_count = 0
        
        ref_dir = Path(self.ref)
        ref_names = [int(file.stem) for file in ref_dir.iterdir() if file.is_file() and file.suffix == '.pdf']
        print(f"ğŸ“ å¼•ç”¨æ–‡ä»¶å¤¹ä¸­æ‰¾åˆ°{len(ref_names)}ä¸ªPDFæ–‡ä»¶: {sorted(ref_names)}")
        
        citations_to_text = get_citation_markers(self.doc)
        print(f"ğŸ” å¼€å§‹æ ¸æŸ¥{len(citations_to_text)}ä¸ªå¼•ç”¨æ ‡è®°...")
        
        for i, (citation, text) in enumerate(citations_to_text):
            print(f"\næ ¸æŸ¥å¼•ç”¨ {i+1}/{len(citations_to_text)}: {citation}")
            
            # æ£€æŸ¥å¼•ç”¨çš„æ–‡çŒ®æ˜¯å¦éƒ½æœ‰å¯¹åº”çš„PDFæ–‡ä»¶
            missing_refs = set(citation) - set(ref_names)
            if missing_refs:
                print(f"â­ï¸  è·³è¿‡å¼•æ–‡{citation}ï¼Œå› ä¸ºä»¥ä¸‹æ–‡çŒ®çš„PDFæ–‡ä»¶ç¼ºå¤±: {sorted(missing_refs)}")
                skipped_count += 1
                continue
            
            try:
                # æ„å»ºPDFæ–‡ä»¶è·¯å¾„
                reference_paths = [ref_dir / f"{index}.pdf" for index in citation]
                
                # æ£€æŸ¥æ‰€æœ‰æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                missing_files = [path for path in reference_paths if not path.exists()]
                if missing_files:
                    print(f"â­ï¸  è·³è¿‡å¼•æ–‡{citation}ï¼Œå› ä¸ºæ–‡ä»¶ä¸å­˜åœ¨: {[f.name for f in missing_files]}")
                    skipped_count += 1
                    continue
                
                print(f"ğŸ“– åŠ è½½PDFæ–‡ä»¶: {[f'{index}.pdf' for index in citation]}")
                references = load_pdf(reference_paths)
                
                if not references.strip():
                    print(f"âš ï¸  è·³è¿‡å¼•æ–‡{citation}ï¼Œå› ä¸ºPDFå†…å®¹ä¸ºç©º")
                    skipped_count += 1
                    continue
                
                print(f"ğŸ¤– ä½¿ç”¨AIæ¨¡å‹éªŒè¯å¼•æ–‡...")
                prompt = self.prompt.format(text, references)
                response = self.call_model(self.model, prompt)
                checked_count += 1
                
                if "<æ˜¯>" == response:
                    print(f"âœ… å¼•æ–‡{citation}æ ¸æŸ¥æ— è¯¯")
                elif "å¦" in response:
                    bad_count += 1
                    print(f"âŒ å¼•æ–‡{citation}æ£€æµ‹é”™è¯¯: {response}")
                else:
                    print(f"âš ï¸  å¼•æ–‡{citation}æ£€æµ‹ç»“æœä¸æ˜ç¡®: {response}")
                    
            except Exception as e:
                print(f"âŒ å¤„ç†å¼•æ–‡{citation}æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
                skipped_count += 1
                continue
                
        print("\n" + "="*50)
        print("ğŸ“Š æ ¸æŸ¥ç»Ÿè®¡ç»“æœ:")
        print(f"   æ€»å¼•ç”¨æ ‡è®°æ•°: {len(citations_to_text)}")
        print(f"   å·²æ ¸æŸ¥: {checked_count}")
        print(f"   è·³è¿‡: {skipped_count}")
        print(f"   æ£€æµ‹ä¸ºé”™è¯¯: {bad_count}")
        print(f"   æ£€æµ‹ä¸ºæ­£ç¡®: {checked_count - bad_count}")
        
        if checked_count > 0:
            accuracy_rate = ((checked_count - bad_count) / checked_count) * 100
            print(f"   å‡†ç¡®ç‡: {accuracy_rate:.1f}%")
        
        if bad_count == 0 and checked_count > 0:
            print("\nğŸ‰ æ‰€æœ‰æ ¸æŸ¥çš„å¼•æ–‡éƒ½æ˜¯æ­£ç¡®çš„!")
        elif bad_count > 0:
            print(f"\nâš ï¸  å‘ç°{bad_count}ä¸ªå¯èƒ½å­˜åœ¨é—®é¢˜çš„å¼•æ–‡ï¼Œè¯·ä»”ç»†æ£€æŸ¥")

    def verify_citation_sentences_lightweight(self):
        """è½»é‡çº§æ ¸æŸ¥å¼•æ–‡ä¸æ–‡çŒ®çš„å¯¹åº”å…³ç³» - ä½¿ç”¨arXivå…ƒæ•°æ®è€ŒéPDFä¸‹è½½"""
        print("ç¬¬3æ­¥(è½»é‡ç‰ˆ): ä½¿ç”¨arXivå…ƒæ•°æ®éªŒè¯å¼•æ–‡")
        
        titles = get_reference_titles(self.doc)
        citations_to_text = get_citation_markers(self.doc)
        
        print(f"å‚è€ƒæ–‡çŒ®æ•°é‡: {len(titles)}")
        print(f"å¼•ç”¨æ ‡è®°æ•°é‡: {len(citations_to_text)}")
        print("æ­£åœ¨éªŒè¯...")
        
        # ä½¿ç”¨è½»é‡çº§éªŒè¯
        verification_results = batch_verify_citations_lightweight(citations_to_text, titles, self.model)
        
        # ç»Ÿè®¡ç»“æœ
        verified_count = sum(1 for r in verification_results if r['status'] == 'verified')
        skipped_count = sum(1 for r in verification_results if r['status'] == 'skipped')
        error_count = sum(1 for r in verification_results if r['status'] == 'error')
        correct_count = sum(1 for r in verification_results if r['status'] == 'verified' and '<æ˜¯>' in r.get('result', ''))
        incorrect_count = sum(1 for r in verification_results if r['status'] == 'verified' and 'å¦' in r.get('result', ''))
        
        print(f"\néªŒè¯ç»“æœ:")
        print(f"  å·²éªŒè¯: {verified_count}")
        print(f"  æ­£ç¡®: {correct_count}")
        print(f"  æœ‰é—®é¢˜: {incorrect_count}")
        print(f"  è·³è¿‡: {skipped_count}")
        
        if verified_count > 0:
            accuracy_rate = (correct_count / verified_count) * 100
            print(f"  å‡†ç¡®ç‡: {accuracy_rate:.1f}%")
        
        # æ˜¾ç¤ºæœ‰é—®é¢˜çš„å¼•ç”¨
        if incorrect_count > 0:
            print(f"\nå‘ç°é—®é¢˜çš„å¼•ç”¨:")
            problem_count = 0
            for result in verification_results:
                if result['status'] == 'verified' and 'å¦' in result.get('result', ''):
                    problem_count += 1
                    if problem_count <= 5:  # åªæ˜¾ç¤ºå‰5ä¸ªé—®é¢˜
                        citation = result['citation']
                        ai_result = result.get('result', '')
                        reason = ai_result.replace('<å¦:', '').replace('>', '').strip()
                        print(f"  å¼•ç”¨{citation}: {reason[:80]}...")
                    
            if problem_count > 5:
                print(f"  ...è¿˜æœ‰{problem_count - 5}ä¸ªé—®é¢˜")
        
        # ç®€åŒ–çš„ä¼˜åŠ¿è¯´æ˜
        if verified_count > 0:
            print(f"\nè½»é‡çº§éªŒè¯ä¼˜åŠ¿: å¿«é€Ÿã€èŠ‚çœç©ºé—´ã€åŸºäºæœ€æ–°æ•°æ®")
        
        return verification_results


if __name__ == "__main__":
    load_dotenv()

    parser = argparse.ArgumentParser()
    parser.add_argument("--model", type=str, default="glm-4-flash")
    parser.add_argument("--prompt", type=str, default="../reference_agent/prompts/agent_prompt")
    parser.add_argument("--doc", type=str, default="c:\\Users\\seer\\Desktop\\reference_agent\\data\\docs\\abb.pdf")
    parser.add_argument("--ref", type=str, default="../data/references")
    parser.add_argument("--lightweight", action="store_true", default=False, help="ä½¿ç”¨è½»é‡çº§æ¨¡å¼,è·³è¿‡PDFä¸‹è½½,ä»…ä½¿ç”¨arXivå…ƒæ•°æ®éªŒè¯")
    parser.add_argument("--skip-download", action="store_true", default=False, help="è·³è¿‡PDFä¸‹è½½æ­¥éª¤")
    parser.add_argument("--skip-pdf-verify", action="store_true", default=False, help="è·³è¿‡PDFéªŒè¯æ­¥éª¤")
    args = parser.parse_args()

    agent = Agent(args.model, args.prompt, args.doc, args.ref)
    
    # ç¬¬1æ­¥ï¼šæ€»æ˜¯æ‰§è¡Œå¼•æ–‡æ•°é‡æ£€æŸ¥
    agent.verify_citations_referenced()
    print("------------åˆ†å‰²çº¿-------------")
    
    if args.lightweight:
        print("ğŸš€ ä½¿ç”¨è½»é‡çº§æ¨¡å¼ - è·³è¿‡PDFä¸‹è½½ï¼Œç›´æ¥ä½¿ç”¨arXivå…ƒæ•°æ®éªŒè¯")
        agent.verify_citation_sentences_lightweight()
    else:
        # ä¼ ç»Ÿæ¨¡å¼
        if not args.skip_download:
            agent.download_literatures()
            print("------------åˆ†å‰²çº¿-------------")
        
        if not args.skip_pdf_verify:
            agent.verify_citation_sentences()
            print("------------åˆ†å‰²çº¿-------------")
        
        # é¢å¤–æ‰§è¡Œè½»é‡çº§éªŒè¯ä½œä¸ºå¯¹æ¯”
        print("ğŸ”„ é™„åŠ æ‰§è¡Œè½»é‡çº§éªŒè¯ä»¥ä¾›å¯¹æ¯”:")
        agent.verify_citation_sentences_lightweight()






